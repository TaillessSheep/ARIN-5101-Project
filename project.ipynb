{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "This notebook uses a **10-class subset** of CIFAR-100 for faster training and experimentation. The dataset is automatically filtered to include only the selected classes, and labels are remapped to 0-9.\n",
    "\n",
    "**Available Subsets:**\n",
    "\n",
    "- **`balanced`** (Balanced & Easy): Distinct shapes and colors - Recommended for quick training\n",
    "  - Classes: beaver, shark, tulip, plate, television, chair, butterfly, bear, mountain, train\n",
    "\n",
    "- **`animals`** (All-Animals Theme): Consistent biological category\n",
    "  - Classes: beaver, whale, trout, fox, wolf, chimpanzee, squirrel, raccoon, turtle, hamster\n",
    "\n",
    "- **`objects`** (Human-Made Objects): Object-focused subset\n",
    "  - Classes: bottle, cup, keyboard, television, chair, table, house, road, pickup truck, train\n",
    "\n",
    "**To switch subsets:** Change `SUBSET_CHOICE = 'balanced'` in the next cell to `'animals'` or `'objects'`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "\n",
    "Load all configuration settings from `config.json`, including training parameters and subset options.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "config_path = Path('config.json')\n",
    "if config_path.exists():\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "    print(f\"Loaded configuration from {config_path}\")\n",
    "\n",
    "# Set the baseline model and report paths\n",
    "model_dir = Path(config['model_dir'])\n",
    "reports_dir = Path(config['reports_dir'])\n",
    "\n",
    "# Extract subset options from config\n",
    "if 'subset_options' in config:\n",
    "    subset_map = {name: opts['classes'] for name, opts in config['subset_options'].items()}\n",
    "    SUBSET_CHOICE = config['subset_choice']\n",
    "    subset_class_name = subset_map[SUBSET_CHOICE]\n",
    "else:\n",
    "    # Fallback if subset_options not in config (for backward compatibility)\n",
    "    print(\"Warning: subset_options not found in config.json. Using default subsets: balanced.\")\n",
    "    SUBSET_CHOICE = 'balanced'\n",
    "    subset_class_name = ['beaver', 'shark', 'tulip', 'plate', 'television', 'chair', 'butterfly', 'bear', 'mountain', 'train']\n",
    "\n",
    "print(f\"\\nConfiguration Summary:\")\n",
    "print(f\"  Subset Choice: {SUBSET_CHOICE}\")\n",
    "if 'subset_options' in config and SUBSET_CHOICE in config['subset_options']:\n",
    "    subset_info = config['subset_options'][SUBSET_CHOICE]\n",
    "    print(f\"  Subset Name: {subset_info['name']}\")\n",
    "    print(f\"  Description: {subset_info['description']}\")\n",
    "    print(f\"  Classes: {subset_info['classes']}\")\n",
    "print(f\"  Learning Rate: {config['learning_rate']}\")\n",
    "print(f\"  Epochs: {config['num_epochs']}\")\n",
    "print(f\"  Batch Size: {config['batch_size']}\")\n",
    "print(f\"  Baseline:\")\n",
    "print(f\"    Model Directory: {model_dir}\")\n",
    "print(f\"    Reports Directory: {reports_dir}\")\n",
    "if 'subset_options' in config and SUBSET_CHOICE in config['subset_options']:\n",
    "    subset_info = config['subset_options'][SUBSET_CHOICE]\n",
    "    print(f\"    Model Name: {subset_info.get('model_name', 'None (will be auto-generated)')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### SubsetFilter Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SubsetFilter(Dataset):\n",
    "    \"\"\"Filter dataset to only include selected classes and remap labels to 0-9.\"\"\"\n",
    "    def __init__(self, dataset, selected_class_names):\n",
    "        self.dataset = dataset\n",
    "        self.selected_classes = selected_class_names\n",
    "        # Map original class names to their indices as used in the dataset\n",
    "        class_name_to_idx = {name: idx for idx, name in enumerate(self.dataset.classes)}\n",
    "        # Find the indices corresponding to the selected class names\n",
    "        self.selected_class_indices = [class_name_to_idx[name] for name in selected_class_names]\n",
    "        # Create mapping from original class name to new class ID (0-9)\n",
    "        self.class_to_idx = {name: new_idx for new_idx, name in enumerate(selected_class_names)}\n",
    "        # Create mapping from original class index (0-99) to new class index (0-9)\n",
    "        self.original_idx_to_new_idx = {\n",
    "            class_name_to_idx[name]: new_idx \n",
    "            for new_idx, name in enumerate(selected_class_names)\n",
    "        }\n",
    "        # Filter indices: only keep samples whose label is in selected_class_indices\n",
    "        self.indices = []\n",
    "        for idx, (_, label) in enumerate(dataset):\n",
    "            if label in self.selected_class_indices:\n",
    "                self.indices.append(idx)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        actual_idx = self.indices[idx]\n",
    "        image, original_label = self.dataset[actual_idx]\n",
    "        # Remap label to 0-9 using the original index to new index mapping\n",
    "        new_label = self.original_idx_to_new_idx[original_label]\n",
    "        return image, new_label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Loading and Preparing the Dataset\n",
    "\n",
    "This section loads the CIFAR-100 dataset, applies the selected subset filter, and creates train/validation/test splits with DataLoaders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(9527)\n",
    "\n",
    "# Note: config, SUBSET_CHOICE, and selected_subset are already loaded from config.json in cell 4\n",
    "\n",
    "def get_train_val_test_loaders(transform, batch_size):\n",
    "    # Load the full training dataset (needed to access class names)\n",
    "    cifar100_full = datasets.CIFAR100(root=\"./data\", train=True, download=True, transform=transform)\n",
    "    \n",
    "    # Print subset information using the loaded dataset's classes\n",
    "    print(f\"Using Subset {SUBSET_CHOICE} with classes: {subset_class_name}\")\n",
    "    print(f\"Class names: {subset_class_name}\")\n",
    "    \n",
    "    # Filter to selected subset\n",
    "    cifar100_filtered = SubsetFilter(cifar100_full, subset_class_name)\n",
    "    print(f\"Filtered dataset classes: {cifar100_filtered.selected_classes}\")\n",
    "    \n",
    "    # Create 80/20 train-validation split with fixed random seed\n",
    "    train_size = int(0.8 * len(cifar100_filtered))\n",
    "    val_size = len(cifar100_filtered) - train_size\n",
    "    cifar100_train, cifar100_val = random_split(\n",
    "        cifar100_filtered, \n",
    "        [train_size, val_size], \n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "    \n",
    "    # Load and filter test dataset\n",
    "    cifar100_test_full = datasets.CIFAR100(root=\"./data\", train=False, download=True, transform=transform)\n",
    "    cifar100_test = SubsetFilter(cifar100_test_full, subset_class_name)\n",
    "    \n",
    "    # Create DataLoaders with batch size 128\n",
    "    train_loader = DataLoader(cifar100_train, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(cifar100_val, batch_size=batch_size)\n",
    "    test_loader = DataLoader(cifar100_test, batch_size=batch_size)\n",
    "    \n",
    "    # Print dataset sizes and class count\n",
    "    print(f\"\\nDataset Statistics:\")\n",
    "    print(f\"  Training set size: {len(cifar100_train)}\")\n",
    "    print(f\"  Validation set size: {len(cifar100_val)}\")\n",
    "    print(f\"  Test set size: {len(cifar100_test)}\")\n",
    "    print(f\"  Number of classes: {len(subset_class_name)}\")\n",
    "    print(f\"  Selected classes: {subset_class_name}\")\n",
    "    \n",
    "    return cifar100_full, cifar100_test, train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the transform and batch_size\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "batch_size = 128\n",
    "\n",
    "# Get the train, valiadation, and test loaders\n",
    "cifar100_full, cifar100_test, train_loader, val_loader, test_loader = get_train_val_test_loaders(transform, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Visualizing Training Images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get one batch from the training loader\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "# Confirm image shape and target type\n",
    "print(f\"Image shape: {images[0].shape}\")\n",
    "print(f\"Image shape matches (3, 32, 32): {images[0].shape == (3, 32, 32)}\")\n",
    "print(f\"Target type: {type(labels[0].item())}\")\n",
    "print(f\"Target is integer: {isinstance(labels[0].item(), int)}\")\n",
    "print(f\"Target value example: {labels[0].item()}\")\n",
    "\n",
    "# Plot 4×4 grid of images with class labels\n",
    "plt.figure(figsize=(10, 10))\n",
    "for index in range(9):\n",
    "    plt.subplot(4, 3, index + 1)\n",
    "    # Transpose from (C, H, W) to (H, W, C) for matplotlib\n",
    "    img = images[index].numpy().transpose(1, 2, 0)\n",
    "    # Get the class name for the label (remapped to 0-9, need to map back to original)\n",
    "    remapped_label = labels[index].item()\n",
    "    plt.imshow(img)\n",
    "    plt.title(subset_class_name[remapped_label], fontsize=12)\n",
    "    plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Standardized Cnn Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "class StandardizedCnn(nn.Module):\n",
    "    def __init__(self, config: dict):\n",
    "        super(StandardizedCnn, self).__init__()\n",
    "        self.name = config['name']\n",
    "\n",
    "        poolSize = 2\n",
    "        kernelSize = 3\n",
    "        padding = 1\n",
    "\n",
    "        # compose the cnn layers\n",
    "        orderedCnnDict = []\n",
    "        convLayerDepths = config['convLayerDepths']\n",
    "        convLayerCount = len(convLayerDepths)\n",
    "        previousLayerDepth = config['input_shape'][0]\n",
    "        for i in range(convLayerCount):\n",
    "            orderedCnnDict.append(\n",
    "                # conv layer. e.g.: ('conv#', nn.Conv2d(3, 32, kernel_size=3, padding=1))\n",
    "                (f'conv{i+1}', nn.Conv2d(previousLayerDepth, convLayerDepths[i], kernel_size=kernelSize, padding=padding))\n",
    "            )\n",
    "            orderedCnnDict.append(\n",
    "                # relu layer. e.g.: ('relu#', nn.ReLU())\n",
    "                (f'relu{i+1}', nn.ReLU())\n",
    "            )\n",
    "            orderedCnnDict.append(\n",
    "                # pool layer. e.g.: ('pool#', nn.MaxPool2d(2, 2))\n",
    "                (f'pool{i+1}', nn.MaxPool2d(poolSize, poolSize))\n",
    "            )\n",
    "            previousLayerDepth = convLayerDepths[i]\n",
    "        \n",
    "        if config['dropout'] is not None and config['dropout'] > 0:\n",
    "            orderedCnnDict.append(\n",
    "                # dropout layer. e.g.: ('dropout#', nn.Dropout(0.2))\n",
    "                (f'dropout', nn.Dropout(config['dropout']))\n",
    "            )\n",
    "        self.features = nn.Sequential(\n",
    "            OrderedDict(orderedCnnDict)\n",
    "        )\n",
    "\n",
    "        # compose the linear layers\n",
    "        featuresX = config['input_shape'][1] // (poolSize ** convLayerCount)\n",
    "        featuresY = config['input_shape'][2] // (poolSize ** convLayerCount)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(convLayerDepths[-1] * featuresX * featuresY, config['linearLayerSize']),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config['linearLayerSize'], config['num_classes'])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Print the model summary and parameter count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_summary(model):\n",
    "    print(model)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, loss_fn, optimizer, device):\n",
    "    \"\"\"Train for one epoch and return loss and accuracy.\"\"\"\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        p = model(X_batch)\n",
    "        loss = loss_fn(p, y_batch)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        y_hat = p.argmax(1)\n",
    "        correct += (y_hat == y_batch).sum().item()\n",
    "        total += y_batch.shape[0]\n",
    "    \n",
    "    avg_loss = np.mean(train_loss)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def validate_epoch(model, val_loader, loss_fn, device):\n",
    "    \"\"\"Validate for one epoch and return loss and accuracy.\"\"\"\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            p = model(X_batch)\n",
    "            loss = loss_fn(p, y_batch)\n",
    "            \n",
    "            val_loss.append(loss.item())\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            y_hat = p.argmax(1)\n",
    "            correct += (y_hat == y_batch).sum().item()\n",
    "            total += y_batch.shape[0]\n",
    "    \n",
    "    avg_loss = np.mean(val_loss)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, config, device):\n",
    "    \"\"\"Train the model and return training history.\"\"\"\n",
    "    # Setup loss and optimizer\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    print(f\"Starting training for {config['num_epochs']} epochs...\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for epoch in range(config['num_epochs']):\n",
    "        # Train\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, loss_fn, optimizer, device)\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_acc = validate_epoch(model, val_loader, loss_fn, device)\n",
    "        \n",
    "        # Store history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"Epoch {epoch+1}/{config['num_epochs']}: \"\n",
    "              f\"Train Loss = {train_loss:.4f}, Train Acc = {train_acc:.2%}, \"\n",
    "              f\"Val Loss = {val_loss:.4f}, Val Acc = {val_acc:.2%}\")\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "    print(\"Training completed!\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "def get_subset_model_name(config):\n",
    "    subset_name = config['subset_choice']\n",
    "    # Get model_name from the selected subset option\n",
    "    if 'subset_options' in config and subset_name in config['subset_options']:\n",
    "        model_name = config['subset_options'][subset_name].get('model_name')  # Can be None\n",
    "    else:\n",
    "        model_name = None\n",
    "    return subset_name, model_name\n",
    "    \n",
    "def find_existing_model_to_load(model_dir, subset_name, model_name):\n",
    "    # Find existing models for this subset\n",
    "    model_to_load = None\n",
    "    # if not config['force_train']:\n",
    "    \n",
    "    # Priority 1: Load model by specific name if specified\n",
    "    if model_name:\n",
    "        named_model_path = model_dir / f'cnn_{subset_name}_{model_name}.pt'\n",
    "        if named_model_path.exists():\n",
    "            model_to_load = named_model_path\n",
    "            print(f\"Found model with specified name: {model_to_load}\")\n",
    "            return model_to_load\n",
    "        else:\n",
    "            print(f\"Model with name '{model_name}' not found. Searching for latest model...\")\n",
    "    \n",
    "    # Priority 2: Load latest model for this subset\n",
    "    existing_models = list(model_dir.glob(f'cnn_{subset_name}_*.pt'))\n",
    "    if existing_models:\n",
    "        model_to_load = max(existing_models, key=os.path.getctime)\n",
    "        print(f\"Loading latest model for subset '{subset_name}': {model_to_load}\")\n",
    "        return model_to_load\n",
    "    \n",
    "    return None\n",
    "\n",
    "def load_history(model_to_load, model, device, reports_dir, subset_name):\n",
    "    # Return if model_to_load is None\n",
    "    if model_to_load is None:\n",
    "        print(f\"No existing models found for subset '{subset_name}'. Will train a new model.\")\n",
    "        return None\n",
    "\n",
    "    # Load the model if found\n",
    "    model.load_state_dict(torch.load(model_to_load, map_location=device))\n",
    "    print(\"Model loaded successfully!\")\n",
    "    \n",
    "    # Try to load training history if available\n",
    "    history_file = reports_dir / f'train_log_{subset_name}.csv'\n",
    "    if history_file.exists():\n",
    "        print(f\"Found training log at {history_file}\")\n",
    "        history_df = pd.read_csv(history_file)\n",
    "        print(f\"Training history has {len(history_df)} epochs\")\n",
    "    else:\n",
    "        print(\"No training history found. Model loaded but history unavailable.\")\n",
    "        history_df = None\n",
    "    return history_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(history, model_dir, model):    \n",
    "    # Save model\n",
    "    subset_name = config['subset_choice']\n",
    "    model_name = model.name\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    model_folder = model_dir / model_name\n",
    "    if not model_folder.exists():\n",
    "        model_folder.mkdir(parents=True)\n",
    "    model_path = model_folder / f'cnn_{subset_name}_{timestamp}.pt'\n",
    "    \n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Model saved to: {model_path}\")\n",
    "    \n",
    "    # Save training history to CSV (subset-specific)\n",
    "    history_df = pd.DataFrame(history)\n",
    "    history_df['epoch'] = range(1, len(history_df) + 1)\n",
    "    history_df = history_df[['epoch', 'train_loss', 'train_acc', 'val_loss', 'val_acc']]\n",
    "\n",
    "    \n",
    "    log_folder = reports_dir / model_name\n",
    "    if not log_folder.exists():\n",
    "        log_folder.mkdir(parents=True)\n",
    "    log_path = log_folder / f'train_log_{subset_name}.csv'\n",
    "\n",
    "    history_df.to_csv(log_path, index=False)\n",
    "    print(f\"Training log saved to: {log_path}\")\n",
    "    return history_df, model_path, log_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config, model_dir, model, device, reports_dir, train_loader, val_loader):\n",
    "    # Get subset, model names, and model_to_load\n",
    "    subset_name, model_name = get_subset_model_name(config)\n",
    "    model_to_load = find_existing_model_to_load(model_dir, subset_name, model_name)\n",
    "    \n",
    "    if not config['force_train']:  # if force_train is False\n",
    "        history_df = load_history(model_to_load, model, device, reports_dir, subset_name)\n",
    "        return history_df\n",
    "\n",
    "    # the force_train (config) should be true to reach here\n",
    "    if model_to_load:\n",
    "        print(f\"Force training enabled. Training new model despite existing model(s).\")\n",
    "    history = train_model(model, train_loader, val_loader, config, device)\n",
    "    history_df, model_path, log_path = save_model(history, model_dir, model)\n",
    "    return history_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_acc_curves(history_df, model_name):\n",
    "    # Plot training curves\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    epochs = history_df['epoch'].values\n",
    "    \n",
    "    # Plot loss curves\n",
    "    ax1.plot(epochs, history_df['train_loss'], 'b-', label='Training Loss', linewidth=2)\n",
    "    ax1.plot(epochs, history_df['val_loss'], 'r-', label='Validation Loss', linewidth=2)\n",
    "    ax1.set_xlabel('Epoch', fontsize=12)\n",
    "    ax1.set_ylabel('Loss', fontsize=12)\n",
    "    ax1.set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "    ax1.legend(fontsize=11)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot accuracy curves\n",
    "    ax2.plot(epochs, history_df['train_acc'], 'b-', label='Training Accuracy', linewidth=2)\n",
    "    ax2.plot(epochs, history_df['val_acc'], 'r-', label='Validation Accuracy', linewidth=2)\n",
    "    ax2.set_xlabel('Epoch', fontsize=12)\n",
    "    ax2.set_ylabel('Accuracy', fontsize=12)\n",
    "    ax2.set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "    ax2.legend(fontsize=11)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot (subset-specific)\n",
    "    subset_name = config['subset_choice']\n",
    "    plot_path = reports_dir/ model_name / f'training_curves_{subset_name}.png'\n",
    "    plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"Training curves saved to: {plot_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def training_report_plot(history_df, model_name):\n",
    "    # Load history if not already loaded\n",
    "    if 'history_df' not in locals() or history_df is None:\n",
    "        subset_name = config['subset_choice']\n",
    "        history_file = reports_dir / model_name / f'train_log_{subset_name}.csv'\n",
    "        if history_file.exists():\n",
    "            history_df = pd.read_csv(history_file)\n",
    "            print(f\"Loaded training history from {history_file}\")\n",
    "        else:\n",
    "            print(f\"No training history available for plotting (looking for {history_file}).\")\n",
    "            history_df = None\n",
    "    \n",
    "    if history_df is not None:\n",
    "        # Plot training curves\n",
    "        plot_loss_acc_curves(history_df, model_name)\n",
    "        \n",
    "        # Display summary statistics\n",
    "        print(\"\\nTraining Summary:\")\n",
    "        print(f\"  Final Train Loss: {history_df['train_loss'].iloc[-1]:.4f}\")\n",
    "        print(f\"  Final Train Accuracy: {history_df['train_acc'].iloc[-1]:.2%}\")\n",
    "        print(f\"  Final Val Loss: {history_df['val_loss'].iloc[-1]:.4f}\")\n",
    "        print(f\"  Final Val Accuracy: {history_df['val_acc'].iloc[-1]:.2%}\")\n",
    "        print(f\"  Best Val Accuracy: {history_df['val_acc'].max():.2%} (Epoch {history_df.loc[history_df['val_acc'].idxmax(), 'epoch']})\")\n",
    "    else:\n",
    "        print(\"Cannot plot: No training history available.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### Missed class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_misclass_samples(misclassified_images, misclassified_preds, misclassified_labels, \n",
    "                          selected_subset, num_samples, rows, cols):\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(12, 12))\n",
    "    fig.suptitle('Misclassified Test Samples (Predicted vs True Label)', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for idx in range(num_samples):\n",
    "        row = idx // rows\n",
    "        col = idx % cols\n",
    "        ax = axes[row, col]\n",
    "        \n",
    "        # Get the image and transpose from (C, H, W) to (H, W, C) for matplotlib\n",
    "        img = misclassified_images[idx].numpy().transpose(1, 2, 0)\n",
    "        \n",
    "        # Get predicted and true labels\n",
    "        pred_label = misclassified_preds[idx].item()\n",
    "        true_label = misclassified_labels[idx].item()\n",
    "        \n",
    "        # Get class names\n",
    "        pred_class_name = selected_subset[pred_label]\n",
    "        true_class_name = selected_subset[true_label]\n",
    "        \n",
    "        # Display image\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(f'Pred: {pred_class_name}\\nTrue: {true_class_name}', \n",
    "                     fontsize=10, color='red' if pred_label != true_label else 'green')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for idx in range(num_samples, 16):\n",
    "        row = idx // 4\n",
    "        col = idx % 4\n",
    "        axes[row, col].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def test_loop_with_misclass(model, test_loader, cifar100_test):\n",
    "    # Compute test accuracy and collect misclassified samples\n",
    "    model.eval()\n",
    "    misclassified_images = []\n",
    "    misclassified_preds = []\n",
    "    misclassified_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            # Find misclassified samples\n",
    "            misclassified_mask = (predicted != labels)\n",
    "            misclassified_images.append(images[misclassified_mask].cpu())\n",
    "            misclassified_preds.append(predicted[misclassified_mask].cpu())\n",
    "            misclassified_labels.append(labels[misclassified_mask].cpu())\n",
    "\n",
    "    # Concatenate all misclassified samples\n",
    "    if misclassified_images:\n",
    "        misclassified_images = torch.cat(misclassified_images, dim=0)\n",
    "        misclassified_preds = torch.cat(misclassified_preds, dim=0)\n",
    "        misclassified_labels = torch.cat(misclassified_labels, dim=0)\n",
    "\n",
    "        print(f\"===============================================\")\n",
    "        print(f\"Total misclassified samples: {len(misclassified_images)}\")\n",
    "        print(f\"Test set size: {len(cifar100_test)}\")\n",
    "        print(f\"Test accuracy: {(len(cifar100_test) - len(misclassified_images)) / len(cifar100_test):.2%}\")\n",
    "        \n",
    "        # Display up to 16 misclassified samples in a 4x4 grid\n",
    "        num_samples = min(16, len(misclassified_images))\n",
    "        plot_misclass_samples(misclassified_images, misclassified_preds, misclassified_labels, \n",
    "                              subset_class_name, num_samples, 4, 4)\n",
    "        \n",
    "    else:\n",
    "        print(\"No misclassified samples found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "# Baseline Cnn\n",
    "- 2 convolution layers\n",
    "    - For example, using Conv2D, ReLU and MaxPool\n",
    "- 1 dense layer with:\n",
    "    - Input layer – flattened features from the convolution layers\n",
    "    - Hidden layer – 128 units\n",
    "    - Output layer – the number of classes in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Model Def\n",
    "Conv: 3 -> 32 -> 64\n",
    "Linear: 128 -> 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseCnn(StandardizedCnn):\n",
    "    config = {\n",
    "        'name': 'baseline',\n",
    "        'input_shape': (3, 32, 32),\n",
    "        'num_classes': 10,\n",
    "        'convLayerDepths': [32, 64],\n",
    "        'linearLayerSize': 128,\n",
    "        'dropout': 0\n",
    "    }\n",
    "    def __init__(self):\n",
    "        super().__init__(self.config)\n",
    "\n",
    "\n",
    "# Print the summary of the baseline model\n",
    "base_model = BaseCnn()\n",
    "print_model_summary(base_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### Training Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"\\nTraining Configuration:\")\n",
    "for key, value in config.items():\n",
    "    if key != 'subset_options':  # Don't print the full subset_options dict\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "if 'subset_options' in config and config['subset_choice'] in config['subset_options']:\n",
    "    subset_info = config['subset_options'][config['subset_choice']]\n",
    "    print(f\"\\n  Selected Subset Info:\")\n",
    "    print(f\"    Name: {subset_info['name']}\")\n",
    "    print(f\"    Description: {subset_info['description']}\")\n",
    "    print(f\"    Classes: {subset_info['classes']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the device, and move the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() \n",
    "                      else 'mps' if torch.backends.mps.is_available()\n",
    "                      else 'cpu')\n",
    "print(f\"Currently using {device} to train...\")\n",
    "\n",
    "# Ensure the the model is moved to the device\n",
    "base_model = base_model.to(device)\n",
    "\n",
    "# Base model - Train or load\n",
    "history_df = train(config, model_dir, base_model, device, reports_dir, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### Plotting Training Curves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot and print the loss and accuracy\n",
    "training_report_plot(history_df, base_model.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "### Display Misclassified Test Samples\n",
    "\n",
    "This section identifies and displays misclassified samples from the test set, showing the predicted label vs the true label for each misclassified image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the testing loop on the base model\n",
    "test_loop_with_misclass(base_model, test_loader, cifar100_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "# Model Improvement\n",
    "- You will progressively enhance the baseline using:\n",
    "- Dropout – Prevent overfitting\n",
    "- Batch Normalization – Stabilize training\n",
    "- Data Augmentation – Improve generalization\n",
    "- You only pick two of the above three improvements\n",
    "- Your performance comparison will include using only one improvement, and using both improvements at the same time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "## Model Def\n",
    "- Data Augmentation\n",
    "    - RandomHorizontalFlip = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedCnn_w_dropout(StandardizedCnn):\n",
    "    config = {\n",
    "        'name': 'improvedWTuning',\n",
    "        'input_shape': (3, 32, 32),\n",
    "        'num_classes': 10,\n",
    "        'convLayerDepths': [64, 64],\n",
    "        'linearLayerSize': 256,\n",
    "        'dropout': 0.2\n",
    "    }\n",
    "    def __init__(self):\n",
    "        super().__init__(self.config)\n",
    "\n",
    "# Initialized the improved cnn model, and ensure the the model is moved to the device\n",
    "improved_model_w_dropout = ImprovedCnn_w_dropout().to(device)\n",
    "print_model_summary(improved_model_w_dropout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add RandomHorizontalFlip=0.5\n",
    "improved_transform_rand_flip = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomHorizontalFlip(p=0.5)\n",
    "])\n",
    "\n",
    "improved_batch_size = 32  # Change the batch size from 128 -> 32\n",
    "\n",
    "# Get the updated loaders\n",
    "cifar100_full, cifar100_test, train_loader, val_loader, test_loader = get_train_val_test_loaders(improved_transform_rand_flip, improved_batch_size)\n",
    "\n",
    "# Initialized the improved cnn model, and ensure the the model is moved to the device\n",
    "improved_model_w_dropout = ImprovedCnn_w_dropout().to(device)\n",
    "\n",
    "# Improved model + Resize + Normalized - Train or load\n",
    "history_df = train(config, model_dir, improved_model_w_dropout, device, reports_dir, train_loader, val_loader)\n",
    "\n",
    "# Plot and print the loss and accuracy\n",
    "training_report_plot(history_df, improved_model_w_dropout.name)\n",
    "\n",
    "# Run the testing loop on the improved model\n",
    "test_loop_with_misclass(improved_model_w_dropout, test_loader, cifar100_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "# Model tuning\n",
    "You will experiment your baseline model with:\n",
    "- Convolution layer depth\n",
    "- For example, using values of 32, 64 and so on\n",
    "- Hidden layer size\n",
    "- For example, using different number of hidden layer units\n",
    "- Adding a third convolutional layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "## Tunning Convolution Layer Depth\n",
    "### Model Def\n",
    "3 -> 64 -> 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedCnn(StandardizedCnn):\n",
    "    config = {\n",
    "        'name': 'improved',\n",
    "        'input_shape': (3, 32, 32),\n",
    "        'num_classes': 10,\n",
    "        'convLayerDepths': [64, 128],\n",
    "        'linearLayerSize': 128,\n",
    "        'dropout': 0\n",
    "    }\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(self.config)\n",
    "\n",
    "# Print the summary of the improved model\n",
    "improved_model = ImprovedCnn()\n",
    "print_model_summary(improved_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialized the improved cnn model, and ensure the the model is moved to the device\n",
    "improved_model = improved_model.to(device)\n",
    "\n",
    "# Improved model - Train or load\n",
    "history_df = train(config, model_dir, improved_model, device, reports_dir, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot and print the loss and accuracy\n",
    "training_report_plot(history_df, improved_model.name)\n",
    "\n",
    "# Run the testing loop on the improved model\n",
    "test_loop_with_misclass(improved_model, test_loader, cifar100_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "## Hidden layer size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "## A third convolutional layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "# Resize (this is not improvement nor tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedCnn_for_resize(StandardizedCnn):\n",
    "    config = {\n",
    "        'name': 'improvedForResize',\n",
    "        'input_shape': (3, 224, 224),\n",
    "        'num_classes': 10,\n",
    "        'convLayerDepths': [64, 64],\n",
    "        'linearLayerSize': 256,\n",
    "        'dropout': 0.2\n",
    "    }\n",
    "    def __init__(self):\n",
    "        super().__init__(self.config)\n",
    "\n",
    "# Initialized the improved cnn model\n",
    "improved_model_for_size = ImprovedCnn_for_resize()\n",
    "print_model_summary(improved_model_for_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize to 224, and normalize the dataset\n",
    "improved_transform_224 = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761])\n",
    "])\n",
    "\n",
    "improved_batch_size = 32  # Change the batch size from 128 -> 32\n",
    "\n",
    "# Get the updated loaders\n",
    "cifar100_full, cifar100_test, train_loader, val_loader, test_loader = get_train_val_test_loaders(improved_transform_224, improved_batch_size)\n",
    "\n",
    "# Ensure the the model is moved to the device\n",
    "improved_model_for_size = improved_model_for_size.to(device)\n",
    "\n",
    "# Improved model + Resize + Normalized - Train or load\n",
    "history_df = train(config, model_dir, improved_model_for_size, device, reports_dir, train_loader, val_loader)\n",
    "\n",
    "# Plot and print the loss and accuracy\n",
    "training_report_plot(history_df, improved_model_for_size.name)\n",
    "\n",
    "# Run the testing loop on the improved model\n",
    "test_loop_with_misclass(improved_model_for_size, test_loader, cifar100_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Arin5101Proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
